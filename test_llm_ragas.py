import os
from dotenv import load_dotenv

import pytest
from datasets import Dataset

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from ragas import evaluate
from ragas.metrics import Faithfulness, ContextRecall, ContextPrecision

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
llm_temperature = 0.6


def test_case_1():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    llm = ChatOpenAI(temperature=llm_temperature, openai_api_key=openai_api_key)

    data = {
        "question": [
            "what is the capital of France?"
        ],
        "ground_truth": [
            "Paris is the capital of France"
        ],
        "answer": [
            "France capital is Paris"
        ],
        "contexts": [
            ["Paris is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants). Paris has 2.234 million inhabitants end 2011. She is the core of Ile de France region (12 million people)."]
        ],
        "reference": ["Paris"],
    }

    dataset = Dataset.from_dict(data)

    result = evaluate(
        dataset=dataset,
        metrics=[Faithfulness(), ContextRecall(), ContextPrecision()],
        llm=llm,
        embeddings=embeddings,
    )
    print(result)
    return result.to_pandas()

# To run the test, you can call the function:
evaluation_df_1 = test_case_1()
evaluation_df_1.to_csv("test_case_1.csv", index=False)
print(f"\nCSV saved to test_case_1.csv. Columns in CSV: {evaluation_df_1.columns.tolist()}")


def test_case_2():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    llm = ChatOpenAI(temperature=llm_temperature, openai_api_key=openai_api_key)

    data = {
        "question": [
            "What is the favorite sport in canada?"
        ],
        "ground_truth": [
            "Canada's most popular sport is Ice Hockey"
        ],
        "answer": [
            "Hockey"
        ],
        "contexts": [
            ["Canada's most popular sport is ice hockey. It's officially recognized as the national winter sport and is widely viewed as a cultural icon in the country. Survey data from The Logit Group indicates that hockey is the favourite sport to watch for Canadians, with around 22% of respondents naming it as their preferred sport."]
        ],
        "reference": ["Ice Hockey"],
    }

    dataset = Dataset.from_dict(data)

    result = evaluate(
        dataset=dataset,
        metrics=[Faithfulness(), ContextRecall(), ContextPrecision()],
        llm=llm,
        embeddings=embeddings,
    )
    print(result)
    return result.to_pandas()


evaluation_df_2 = test_case_2()
evaluation_df_2.to_csv("test_case_2.csv", index=False)
print(f"\nCSV saved to test_case_2.csv. Columns in CSV: {evaluation_df_2.columns.tolist()}")


def test_case_3():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    llm = ChatOpenAI(temperature=llm_temperature, openai_api_key=openai_api_key)

    data = {
        "question": [
            "what is the capital of France?"
        ],
        "ground_truth": [
            "Paris is the capital of France"
        ],
        "answer": [
            "France capital is Poland"
        ],
        "contexts": [
            ["Paris is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants). Paris has 2.234 million inhabitants end 2011. She is the core of Ile de France region (12 million people)."]
        ],
        "reference": ["Poland"],
    }

    dataset = Dataset.from_dict(data)

    result = evaluate(
        dataset=dataset,
        metrics=[Faithfulness(), ContextRecall(), ContextPrecision()],
        llm=llm,
        embeddings=embeddings,
    )
    
    print("Ragas evaluation result (summary):")
    print(result) # This prints the aggregated scores

    evaluation_df = result.to_pandas()
    print("\nDataFrame generated by result.to_pandas():")
    print(evaluation_df) # This prints the DataFrame with all columns including metrics

    return evaluation_df

# To run the test, you can call the function:
evaluation_df_3 = test_case_3()

# Now, save it to CSV
evaluation_df_3.to_csv("test_case_3.csv", index=False)

print(f"\nCSV saved to test_case_3.csv. Columns in CSV: {evaluation_df_3.columns.tolist()}")


def test_case_4():
    # --- Setup ---
    # Check if API key is set before proceeding
    if openai_api_key is None:
        pytest.skip("OPENAI_API_KEY environment variable not set. Skipping test.")

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    llm = ChatOpenAI(temperature=llm_temperature, openai_api_key=openai_api_key)

    data = {
        "question": [
            "what is the capital of France?"
        ],
        "ground_truth": [
            "Paris is the capital of France"
        ],
        "answer": [
            "France capital is Poland" # Intentionally wrong to test Faithfulness
        ],
        "contexts": [
            ["Paris is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants). Paris has 2.234 million inhabitants end 2011. She is the core of Ile de France region (12 million people)."]
        ],
        # "reference": ["Poland"], # This column is not used by Ragas metrics and can be removed
    }

    dataset = Dataset.from_dict(data)

    # --- Evaluation ---
    result = evaluate(
        dataset=dataset,
        metrics=[Faithfulness(), ContextRecall(), ContextPrecision()],
        llm=llm,
        embeddings=embeddings,
    )
    
    print("Ragas evaluation result (summary):")
    print(result) 

    evaluation_df = result.to_pandas()
    print("\nDataFrame generated by result.to_pandas():")
    print(evaluation_df) 

    # --- Assertions ---

    # 1. Assert that the DataFrame is not empty
    assert not evaluation_df.empty, "Evaluation DataFrame should not be empty."

    # 2. Assert that key metric columns exist
    assert 'faithfulness' in evaluation_df.columns, "Faithfulness column missing from results."
    assert 'context_recall' in evaluation_df.columns, "ContextRecall column missing from results."
    assert 'context_precision' in evaluation_df.columns, "ContextPrecision column missing from results."

    # 3. Assert that metric scores are not NaN (meaning they were computed)
    # Use .notna().all() to check if all values in the column are not NaN
    assert evaluation_df['faithfulness'].notna().all(), "Faithfulness score should not be NaN."
    assert evaluation_df['context_recall'].notna().all(), "ContextRecall score should not be NaN."
    assert evaluation_df['context_precision'].notna().all(), "ContextPrecision score should not be NaN."

    # 4. Assert specific score thresholds (adjust values based on your expectations)
    # For 'answer': "France capital is Poland" - Faithfulness should be low
    # For 'contexts': "Paris is the capital of France..." - ContextRecall and ContextPrecision should be high

    # Get the score for the single row
    faithfulness_score = evaluation_df['faithfulness'].iloc[0]
    context_recall_score = evaluation_df['context_recall'].iloc[0]
    context_precision_score = evaluation_df['context_precision'].iloc[0]

    # Example: Assert Faithfulness is below a certain threshold (since the answer is wrong)
    # A low Faithfulness score indicates the answer contains information not supported by the context.
    assert faithfulness_score < 0.5, \
        f"Faithfulness score expected to be low (e.g., < 0.5) but got {faithfulness_score}"

    # Example: Assert ContextRecall is high (assuming the context is relevant to the ground truth)
    # ContextRecall measures how much of the ground truth is present in the contexts.
    assert context_recall_score > 0.8, \
        f"ContextRecall score expected to be high (e.g., > 0.8) but got {context_recall_score}"

    # Example: Assert ContextPrecision is high (assuming the context is precise for the question)
    # ContextPrecision measures how much of the context is relevant to the question.
    assert context_precision_score > 0.8, \
        f"ContextPrecision score expected to be high (e.g., > 0.8) but got {context_precision_score}"


    return evaluation_df

# To run the test with pytest, save this in a file like `test_ragas_metrics.py`
# and simply run `pytest` in your terminal.

# If you want to run it directly from a script (for debugging/manual check):
if __name__ == "__main__":
    # For direct script execution, ensure OPENAI_API_KEY is set in your shell
    # or uncomment the hardcoded key for testing (and remember to remove it!)
    try:
        evaluation_df_4 = test_case_4()
        evaluation_df_4.to_csv("test_case_4.csv", index=False)
        print(f"\nCSV saved to test_case_4.csv. Columns in CSV: {evaluation_df_4.columns.tolist()}")
    except Exception as e:
        print(f"An error occurred during test execution: {e}")