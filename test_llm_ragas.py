import os
from dotenv import load_dotenv

import pytest
from datasets import Dataset

from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from ragas import evaluate
from ragas.metrics import Faithfulness, ContextRecall, ContextPrecision

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
llm_temperature = 0.6

def test_case_4():
    # --- Setup ---
    # Check if API key is set before proceeding
    if openai_api_key is None:
        pytest.skip(
            "OPENAI_API_KEY environment variable not set. Skipping test.")

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    llm = ChatOpenAI(temperature=llm_temperature,
                     openai_api_key=openai_api_key)

    # The `data` dictionary in the test case is used to store the input data for evaluation. It contains keys such as "question", "ground_truth", "answer", and "contexts", each corresponding to a list of strings. These lists represent the questions, ground truth answers, model answers, and contexts for evaluation, respectively. The data is then converted into a `Dataset` object using the `Dataset.from_dict(data)` method for evaluation with specific metrics.
    data = {
        "question": [
            "What is the capital of Canada?",
            "What are the popular sports in Canada?",
            "What are the two official languages of Canada?",
            "How many lakes are in canada?"
        ],
        "ground_truth": [
            "Ottawa is the capital of Canada",
            "Ice Hockey and Lacrosse",
            "English and French and Two official languages in Canada",
            "Over 2 million lakes"
        ],
        "answer": [
            "Ottawa is the capital of Canada",
            "Ice Hockey and Lacrosse",
            "French and English",
            "Under 2 million lakes"
        ],
        "contexts": [
            ["Ottawa is the capital city of Canada. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau River."],
            ["In Canada, ice hockey and lacrosse are the official national sports. Hockey is widely recognized as the most popular sport, followed by soccer and basketball. Other popular sports include baseball, curling, and various other sports."],
            ["Canada has two official languages: English and French"],
            ["Canada is home to over 2 million lakes, representing a significant portion of the world's freshwater resources and a large percentage of Canada's land area. Specifically, it's estimated that nearly 9% of Canada's total land surface is covered by freshwater, according to a report."]
        ],
    }

    dataset = Dataset.from_dict(data)

    # --- Evaluation ---
    result = evaluate(
        dataset=dataset,
        metrics=[Faithfulness(), ContextRecall(), ContextPrecision()],
        llm=llm,
        embeddings=embeddings,
    )

    print("Ragas evaluation result (summary):")
    print(result)

    evaluation_df = result.to_pandas()
    print("\nDataFrame generated by result.to_pandas():")
    print(evaluation_df)

    # --- Assertions ---
    # Assert that the DataFrame is not empty
    assert not evaluation_df.empty, "Evaluation DataFrame should not be empty."

    # Assert that key metric columns exist
    assert 'faithfulness' in evaluation_df.columns, "Faithfulness column missing from results."
    assert 'context_recall' in evaluation_df.columns, "ContextRecall column missing from results."
    assert 'context_precision' in evaluation_df.columns, "ContextPrecision column missing from results."

    # Assert that metric scores are not NaN (meaning they were computed)
    # Use .notna().all() to check if all values in the column are not NaN
    assert evaluation_df['faithfulness'].notna(
    ).all(), "Faithfulness score should not be NaN."
    assert evaluation_df['context_recall'].notna(
    ).all(), "ContextRecall score should not be NaN."
    assert evaluation_df['context_precision'].notna(
    ).all(), "ContextPrecision score should not be NaN."

    # Assert specific score thresholds (adjust values based on your expectations)
    # For 'answer': "France capital is Poland" - Faithfulness should be low
    # For 'contexts': "Paris is the capital of France..." - ContextRecall and ContextPrecision should be high

    # Get the score for the single row
    faithfulness_score = evaluation_df['faithfulness'].iloc[0]
    context_recall_score = evaluation_df['context_recall'].iloc[0]
    context_precision_score = evaluation_df['context_precision'].iloc[0]

    # Example: Assert Faithfulness is below a certain threshold (since the answer is wrong)
    # A low Faithfulness score indicates the answer contains information not supported by the context.
    assert faithfulness_score >= 0.0, f"Faithfulness score expected to be high (e.g., >= 0.0) but got {faithfulness_score}"

    # Example: Assert ContextRecall is high (assuming the context is relevant to the ground truth)
    # ContextRecall measures how much of the ground truth is present in the contexts.
    assert context_recall_score >= 0.0, f"ContextRecall score expected to be high (e.g., >= 0.0) but got {context_recall_score}"

    # Example: Assert ContextPrecision is high (assuming the context is precise for the question)
    # ContextPrecision measures how much of the context is relevant to the question.
    assert context_precision_score >= 0.0, f"ContextPrecision score expected to be high (e.g., >= 0.0) but got {context_precision_score}"

    return evaluation_df

try:
    evaluation_df_4 = test_case_4()
    evaluation_df_4.to_csv("test_case_4.csv", index=False)
    print(
        f"\nCSV saved to test_case_4.csv. Columns in CSV: {evaluation_df_4.columns.tolist()}")
except Exception as e:
    print(f"An error occurred during test execution: {e}")
